<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Теорвер для алгоритмистов - Алгоритмика</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../pandoc.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link href="data:image/x-icon;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfjBggQFxAS4ilBAAACgElEQVRIx52VS0iUYRSGn/8fzbxMIYoRKSZKi2wGwo2FxRQZIlGbaGUF0b52LWbTLlx0W7VyEZiBWkE5mraRRrJFRUwolQw6zQTeuoiU423eFuk0M//nhJ7dd+Z7n/8932HOscgIgU0lR/Cxn3LKgGlijDJIkAgJiywhZMujVo0qrsyIa1St8shWFnmp/IooW0TkV6mMYuRVQCv6X6woIK+QQ16vkPP2M/WZICHVpyGEvCb5vE7olH6bEV6l1R4w3RpQkYoVNJcSWHsLIUt+rTpvLOuSELqihAmwKr8sgZBHE2aX5UKoRmNmDxPyCBtoodLU1m5yaADC9Jr7XkkLoCqNmPAx1eqqnqhAqEHfzB5GVGXTQLUJ/5xZznGcOuAtQbOHahpsfOQ5f5mng8McZAdnsVigkyUTIA8fGjZ561OpuiRJY6oR2qU35iKGbSqc4GU6qObYmstmYIrH5iIq0JIT+17lupU8BVUsVKsvJgdLtgnbxXZOJ091HAU+0W+0YCnGnvRUlCa+42OdbRHiA3CSbtyZ+q85RDMBfcxxjZKUf5uPO4wwzGsaMwFR1JZe1Jx8uuB4mBtC6LJzXLTZDLKYihziM+fJzfjQGfYC/XxMTy8yaDNEOLWBDznAIcdT7aMRiPI0PR1myGacnn+Zd7ygiUIHwEUjuUAnsdR0D+M20E7k7/knt5ki39guN9uAEPeIr6citIPrOsxQiC9hveIuLylkkgXyKUsR/2CAB8yyEzdhJiliN3aCmzwCCwSl3FfzDL9wYZFAuClJAcSZJoELALFKAWVYvVxkdm3NbDRUs0TqUM021jeU15s2w9YXSxKy9dWWRGx6uVpOyObW+x/B+LEV0hF3cAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAxOS0wNi0wOFQxNjoyMzoxNiswMjowMLEfSBUAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMTktMDYtMDhUMTY6MjM6MTYrMDI6MDDAQvCpAAAAV3pUWHRSYXcgcHJvZmlsZSB0eXBlIGlwdGMAAHic4/IMCHFWKCjKT8vMSeVSAAMjCy5jCxMjE0uTFAMTIESANMNkAyOzVCDL2NTIxMzEHMQHy4BIoEouAOoXEXTyQjWVAAAAAElFTkSuQmCC" rel="icon" type="image/x-icon" />

  <!-- Yandex.Metrika counter -->
  <script type="text/javascript">
     (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
     m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
     (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

     ym(53961409, "init", {
          clickmap:true,
          trackLinks:true,
          accurateTrackBounce:true,
          webvisor:true
     });
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/53961409" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
  <!-- /Yandex.Metrika counter -->

  <script>
  // index page width
  document.addEventListener("DOMContentLoaded", function(event) {
    document.getElementsByClassName('contents')[0].parentElement.style.width = "860px";
    document.getElementsByTagName('h1')[0].style.marginRight = "40px";
  });
  </script>
</head>
<body>
<div id='header'>
    <a href='https://algorithmica.org/ru/'><div id='logo'>Алгоритмика</div></a>
    <div id='links'>
        <a href='https://github.com/algorithmica-org/ru/edit/master/probability.md'>Редактировать</a>
        <a href='https://github.com/algorithmica-org/ru/commits/master/probability.md'>История</a>
    </div>
</div>
<div class="migration"><a href='https://ru.algorithmica.org/cs/'>Сайт переезжает.</a> Большинство статей уже перенесено на новую версию.<br>Скоро добавим автоматические переходы, но пока обновленную версию этой статьи можно найти там.</div>
<h1 id="теорвер-для-алгоритмистов">Теорвер для алгоритмистов</h1>
<p>В вузах теорию вероятностей очень долго аксиоматизируют перед тем, как перейти к чему-то полезному. Сейчас продемонстрируем, зачем.</p>
<p>[Парадокс Бертрана](https://ru.wikipedia.org/wiki/%D0%9F%D0%B0%D1%80%D0%B0%D0%B4%D0%BE%D0%BA%D1%81_%D0%91%D0%B5%D1%80%D1%82%D1%80%D0%B0%D0%BD%D0%B0_(%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C%29):</p>
<blockquote>
<p>Рассмотрим равносторонний треугольник, вписанный в окружность. Наудачу выбирается хорда окружности. Какова вероятность того, что выбранная хорда длиннее стороны треугольника?</p>
</blockquote>
<p>Оказывается, можно придумать как минимум три способа решать задачу, которые все выглядят адекватно, но при этом дают разные ответы.</p>
<ol type="1">
<li><p>Наудачу выберем две точки на окружности и проведём через них хорду. Чтобы посчитать искомую вероятность, представим, что треугольник повёрнут так, что одна из его вершин совпадает с концом хорды. Заметим, что если другой конец хорды лежит на дуге между двумя другими вершинами треугольника, то длина хорды больше стороны треугольника. Длина рассмотренной дуги равна трети длины окружности, значит искомая вероятность равна <span class="math inline">\(\frac13\)</span>.</p></li>
<li><p>Зафиксируем радиус окружности, наудачу выберем точку на радиусе. Построим хорду, перпендикулярную зафиксированному радиусу, проходящую через выбранную точку. Для нахождения искомой вероятности, представим, что треугольник повёрнут так, что одна из его сторон перпендикулярна зафиксированному радиусу. Хорда длиннее стороны треугольника, если её центр ближе к центру, чем точка пересечения треугольника с зафиксированным радиусом. Сторона треугольника делит пополам радиус, следовательно вероятность выбрать хорду длиннее стороны треугольника <span class="math inline">\(\frac12\)</span>.</p></li>
<li><p>Выберем наудачу произвольную точку внутри круга и построим хорду с центром в выбранной точке. Хорда длиннее стороны равностороннего треугольника, если выбранная точка находится внутри круга, вписанного в треугольник. Площадь вписанного круга есть <span class="math inline">\(\frac14\)</span> от площади большего, значит исходная вероятность равна <span class="math inline">\(\frac14\)</span>.</p></li>
</ol>
<p>Как мы увидели, от формального определения «случайной хорды» непосредственно зависит ответ. Каждый раз, когда в истории математики появляется подобный приводящий к протеворечиям парадокс, математики паникуют и начинают всё формализовывать и аксиоматизировать. Так появилась теория вероятностей.</p>
<h2 id="аксиоматика">Аксиоматика</h2>
<p><em>Случайной переменной</em> или <em>случайной величиной</em> называется переменная, принимающая различые значения случайно. Сама по себе, случайная переменная только описывает возможные состояния; она должна быть связана с <em>вероятностным распределением</em>, которые указывает, насколько вероятно каждое из состояний.</p>
<p>Например, можно ввести случайную переменную, соответствующую исходу броска шестигранного кубика — она принимает значения из <em>вероятностного пространства</em> <span class="math inline">\(\Omega = \{1, 2, 3, 4, 5, 6\}\)</span>, причём с равной вероятностью: <span class="math inline">\(p(x=i) = \frac{1}{6}\)</span>.</p>
<h3 id="непрерывные-распределения">Непрерывные распределения</h3>
<p>Случайные переменные могут быть не только дискретными, но и непрерывными. Например, мы можем ввести величину: случайное число от 0 до 1.</p>
<p>Плотность вероятности.</p>
<h3 id="ожидание-и-дисперсия">Ожидание и дисперсия</h3>
<h3 id="нормальное-распределение">Нормальное распределение</h3>
<h3 id="парадокс-дней-рождений">Парадокс дней рождений</h3>
<h3 id="условные-вероятности">Условные вероятности</h3>
<h3 id="теория-информации">Теория информации</h3>
<h3 id="section"></h3>
<p>Случайные переменные могут быть дискретными или непрерывными. Дискретные переменные имеют конечное (броски кубика) или счётное количество состояний, а непрерывные переменные принимают значения на континууме.</p>
<p>Функцию X : R будем называть случайной величиной. Она сопостав-</p>
<p>ляет каждому элементарному исходу x какое-то число.</p>
<p>Например, мы можем ввести случайную переменную для исходов броска шестигранного кубика.</p>
<p>Случайные переменные могут быть дискретными или непрерывными. Дискретные переменные имеют конечное или счётное количество состояний. Непрерывные переменные принимают действительные значения.</p>
<p>Вероятностным пространством <span class="math inline">\(\Omega\)</span> называют какое-то множество элементарных исходов. Это, в общем-то, любое множество: например, <span class="math inline">\(\{1, 2, 3, 4, 5, 6\}\)</span> - исходы на кубике, или <span class="math inline">\(\{(x, y) | 1 \leq x, y \leq 6\}\)</span> - исходы на двух последовательно брошенных кубиках.</p>
<p>Событием <span class="math inline">\(A\)</span> в этом пространстве называется некоторое подмножество вероятностного пространства <span class="math inline">\(\Omega\)</span>, например, пустое, или само <span class="math inline">\(\Omega\)</span>, или <span class="math inline">\(\{1, 2, 6\}\)</span> — подмножество <span class="math inline">\(\{1, 2, 3, 4, 5, 6\}\)</span>.</p>
<p>События можно пересекать, объединять, дополнять — они же подмножества.</p>
<p>На некоторых событиях (в том числе элементарных исходах) определена функция <span class="math inline">\(P\)</span>, которую называют вероятностью. Она удовлетворяет следующим условиям:</p>
<ol type="1">
<li><p><span class="math inline">\(P(A \cup B) = P(A) + P(B)\)</span>, если они не пересекаются (<span class="math inline">\(A \cap B = \emptyset\)</span>)</p></li>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span></p></li>
</ol>
<p>Чаще всего (если ничего не указывает на обратное) на элементарных исходах вероятности равны — это называется равномерным распределением.</p>
<p>Во всех этих задачах первого раздела в решениях нужно первым делом описать вероятностное пространство.</p>
%
<p>% % TODO куда-то передвинуть % Андрей, Серёжа и Лёша едут в электричке из Долгопрудного. Отсеки по 6 человек, по 12 в вагоне, вагонов 9. Они хотели бы найти места, где они могут сесть втроем. Они заглянули в вагон и увидели там 7 свободных мест. Они хотят узнать, имеет ли смысл ходить по поезду в поиске свободного места. Помогите им, посчитав матожидание числа вагонов (вероятность найти место?), которые им нужно пройти.</p>
<p>% TODO: какое-то из чисел нужно пофиксить так, чтобы ответ был 1/2 Андрей и Серёжа играют в игру. Они подбрасывают монетку <span class="math inline">\(N\)</span> раз. Если орлов выпало <span class="math inline">\(K\)</span> и больше, то побеждает Андрей. В противном случае побеждает Серёжа (ничья не предусмотрена). С какой вероятностью победит Серёжа?</p>
<ol type="a">
<li><p>N = 3, K = 2</p></li>
<li><p>N = 99, K = 50</p></li>
<li><p>N = 100, K = 50</p></li>
</ol>
<p>Каждый школьник на сборах изучает хотя бы один из трех языков: Java, Python и C++. Вероятности, что случайно выбранный школьник изучает соответственно Java, Python и C++ равны <span class="math inline">\(0.4\)</span>, <span class="math inline">\(0.5\)</span> и <span class="math inline">\(0.6\)</span>. Вероятность, что школьник изучает Java и Python, равна <span class="math inline">\(0.2\)</span>. Вероятность, что школьник изучает Python и C++, равна <span class="math inline">\(0.3\)</span>. Вероятность, что школьник изучает C++ и Java, равна <span class="math inline">\(0.2\)</span>.</p>
<p>Найдите вероятность, что школьник изучает все три языка программирования. Найдите вероятность того, что школьник изучает только C++.</p>
<p>Петя случайным образом разбивает число 10 на сумму трех слагаемых (целые, больше нуля). Опишите вероятностное пространство и вычислите вероятность того, что среди слагаемых будет число 4. Порядок слагаемых в разложении важен.</p>
<p>На шахматной доске размера <span class="math inline">\(n \times n\)</span> случайно размещают <span class="math inline">\(n\)</span> ладей. Найдите вероятность следующих событий:</p>
<ol type="a">
<li><p><span class="math inline">\(A = \{\)</span>ладьи не бьют друг друга<span class="math inline">\(\}\)</span></p></li>
<li><p><span class="math inline">\(B = \{\)</span>ладьи не бьют друг друга, и на главной диагонали нет никаких фигур<span class="math inline">\(\}\)</span></p></li>
</ol>
Докажите, что вероятность того, что на выборах с участием двух кандидатов, в которых первый набрал <span class="math inline">\(p\)</span> голосов, а второй набрал <span class="math inline">\(q \leq p\)</span>, первый будет опережать второго в течение всего времени подсчета, равна <span class="math inline">\(\frac{p-q}{p+q}\)</span>. 
<p>В этих задачах используется красивый прием — вероятность равна отношению каких-нибудь площадей.</p>
<p>Случайная точка <span class="math inline">\(A\)</span> имеет равномерное распределение в прямоугольнике со сторонами <span class="math inline">\(1\)</span> и <span class="math inline">\(2\)</span>. Найдите вероятность следующих событий:</p>
<ol type="a">
<li><p>расстояние от точки <span class="math inline">\(A\)</span> до любой стороны прямоугольника не превосходит <span class="math inline">\(0.1\)</span></p></li>
<li><p>расстояние от точки <span class="math inline">\(A\)</span> до ближайшей большей стороны прямоугольнике меньше, чем до ближайшей меньшей стороны.</p></li>
</ol>
<p><span class="math inline">\(X\)</span> и <span class="math inline">\(Y\)</span> равномерно распределены на отрезке <span class="math inline">\([0, 1]\)</span>. Какая вероятность того, что <span class="math inline">\(X^2 + Y^2 \leq 1\)</span>?</p>
<p>Петя и Вася договорились встретиться с 12:00 до 13:00, но не договорились в какое время, поэтому каждый из них решил прийти в случайное время, подождать 10 минут и уйти. С какой вероятностью они встретятся?</p>
Найти вероятность того, что из трех наудачу взятых отрезков длиной не более, чем 1, можно составить треугольник. 
<p>По определению, вероятность <span class="math inline">\(A\)</span> при условии <span class="math inline">\(B\)</span> равна <span class="math inline">\(P(A | B) = \frac{P(A \cap B)}{P(B)}\)</span>. Это называется условной вероятностью, и в бытовом смысле это вероятность события <span class="math inline">\(A\)</span>, если мы уже знаем, что событие <span class="math inline">\(B\)</span> точно произошло (мы как бы сужаем вероятностное пространство до <span class="math inline">\(B\)</span>, отсюда и формула).</p>
<p>По определению, <span class="math inline">\(A \perp B\)</span> (<span class="math inline">\(A\)</span> независимо с <span class="math inline">\(B\)</span>), если <span class="math inline">\(P(A \cap B) = P(A) P(B)\)</span>. Это совпадает с бытовым понятием независимости событий. Если монетку кидают несколько раз, то считается, что эти броски независимы.</p>
<p>Помещение освещается фонарем с тремя лампами. Вероятность перегорания одной лампы в течение года равна <span class="math inline">\(0.3\)</span>. Найдите вероятность того, что в течение года хотя бы одна лампа не перегорит.</p>
<ol type="a">
<li>Докажете формулу Байеса: <span class="math display">\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]</span></li>
</ol>
<p>По смыслу это очень крутая формула — обычно ты знаешь вероятность событий при каких-то условиях, а вот вероятности выполнения этих условий неочевидны. С помощью этих формул можно посчитать вероятность условий при условии выполнения этих событий.</p>
<ol start="2" type="a">
<li>Докажите формулу полной вероятности: <span class="math display">\[P(A) = P(A|B_1)P(B_1) + ... + P(A|B_n)P(B_n)\]</span> при условии <span class="math display">\[\Omega = B_1 \cup ... \cup B_n (B_i \cap B_j = \emptyset)\]</span></li>
</ol>
<p>Это важная и интуитивно понятная формула — событие просто разбивается на непересекающиеся подслучаи, считаются вероятности событий в этих случаях, и усредняется с весами, равными вероятностям этих случаев.</p>
<p>Из 30 стрелков 12 попадает в цель с вероятностью 0,6, 8 - с вероятностью 0,5 и 10 – с вероятностью 0,7. Наудачу выбранный стрелок произвел выстрел, поразив цель. К какой из этих трех групп вероятнее всего принадлежал этот стрелок? Найдите вероятность его принадлежности к этой группе (при условии, что он действительно попал).</p>
% Я не понял ничего про эту задачу %
<p>% Вы опрашиваете людей на выходе из избирательного участка, кто за кого проголосовал. Всего есть два кандидата — А и Б. Вы опросили 100 людей, из которых 55 проголосовали за А. С какой вероятностью кандидат А победит?</p>
<p>Привести примеры, показывающие, что равенства</p>
<p><span class="math display">\[P(B|A) + P(B|\overline A) = 1\]</span></p>
<p><span class="math display">\[P(B|A) + P(\overline B|\overline A) = 1\]</span></p>
<p>неверны.</p>
<p>События <span class="math inline">\(A_1\)</span>, …, <span class="math inline">\(A_k\)</span> по определению независимы в совокупности, если для любого их подмножества <span class="math inline">\(A_{i_1}\)</span>, …, <span class="math inline">\(A_{i_t}\)</span> верно равенство <span class="math inline">\(P(A_{i_1}, ..., A_{i_t}) = P(A_{i_1}) \times ... \times P(A_{i_t})\)</span>).</p>
<p>Приведите пример трех попарно независимых событий, которы, тем не менее, в совокупности зависимы.</p>
Пусть <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span> – попарно независимые равновероятные события, причем <span class="math inline">\(A \cap B \cap C = \emptyset\)</span>. Найдите максимально возможное значение <span class="math inline">\(P(A)\)</span>. 
<p>Функцию <span class="math inline">\(X: \Omega \rightarrow\mathbb{R}\)</span> будем называть случайной величиной. Она сопоставляет каждому элементарному исходу какое-то число. Тогда можно определить и <span class="math inline">\(P(X \in A)\)</span> - вероятность, что случайная величина <span class="math inline">\(X\)</span> лежит в каком-то подмножестве действительных чисел <span class="math inline">\(A\)</span>, потому что <span class="math inline">\(X \in A\)</span> — это событие в вероятностом пространстве <span class="math inline">\(\Omega\)</span>.</p>
<p>Функцией распределения называют <span class="math inline">\(F_X(x) = P(X \leq x)\)</span>.</p>
<p>Плотностью распределения называют либо <span class="math inline">\(\rho_X(k) = P(X = k)\)</span> (в дискретном случае), либо <span class="math inline">\(\rho_X(x) = F_X(x)&#39;\)</span> — производную функции распределения.</p>
<p>Найдите и нарисуйте функции распределения и плотности следующих случайных величин:</p>
<ol type="a">
<li><p><span class="math inline">\(X = 1\)</span></p></li>
<li><p>$X =</p>
<span class="math display">\[\begin{cases} 
 0, &amp; p \\
 1, &amp; 1-p 
\end{cases}\]</span>
<p>$</p></li>
<li><p><span class="math inline">\(X \sim U[0, 1]\)</span> (<span class="math inline">\(X\)</span> равномерно распределена на отрезке <span class="math inline">\([0, 1]\)</span>)</p></li>
</ol>
<p>Петя кидает монету, с вероятностью <span class="math inline">\(p\)</span> выпадает орел и он прекращает кидать ее, с вероятностью <span class="math inline">\(1-p\)</span> выпадает решка и он кидает ее еще раз. Пусть <span class="math inline">\(X\)</span> - это количество бросков. Найдите <span class="math inline">\(\rho_X(x)\)</span> и <span class="math inline">\(F_X(x)\)</span>.</p>
%
% % Это лучше рассказать просто так % Мост может выдержать до <span class="math inline">\(k\)</span> машин. Чтобы проехать по нему, требуется 1 час. В каждую очень малую единицу времени заезжает машина с вероятностью <span class="math inline">\(\lambda\)</span>. Найдите вероятность того, что мост рухнет в промежуток времени <span class="math inline">\(t\)</span>, то есть по нему за это время проедет более <span class="math inline">\(k\)</span> машин. 
<p>Мат. ожиданием случайной величины <span class="math inline">\(X\)</span>, которая принимает значения <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span>, … называют <span class="math display">\[E[X] = a_0 P(X = a_0) + a_1 P(X = a_1) + ...  = \sum_{x} x p_X(x) \]</span></p>
<p>Если X принимает несчетное число значение (например, равномерное распределение на отрезке <span class="math inline">\([0, 1]\)</span>), то мат. ожидание определяется как интеграл фукнции <span class="math inline">\(x\)</span> умножить на плотность (достаточно понимать, что интеграл - это площадь под графиком этой функции): <span class="math display">\[E[X] = \int\limits_{-\infty}^{\infty} x \rho (x) dx\]</span></p>
<p>Дисперсия определяется как <span class="math inline">\(D[X] = E[(X-E[X])^2]\)</span> — средний квадрат отклонения случайной величины от ее мат. ожидания.</p>
<p>В бытовом смысле мат. ожидание — это среднее значение случайное величины, а именно если взять несколько независимых одинаково распределенных случайных величин, то их среднее действительно будет стремиться к мат. ожиданию (например на физике делают несколько опытов и усредняют ответ именно для этого).</p>
<p>В бытовом смысле дисперсия — это насколько случайная величина шумная.</p>
<p>Самая главная вещь в теории вероятностей: мат. ожидание линейно:</p>
<p><span class="math display">\[\begin{align*}
E[X+Y] &amp; = \sum_{x, y} (x+y) p(x, y)
\\     &amp; = \sum_{x, y} x p(x, y) + \sum_{x, y} y p(x, y)
\\     &amp; = \sum_x x p(x) \sum_y p(y) + \sum_y y p(y) \sum_x p(x)
\\     &amp; = \sum_x x p(x) + \sum_y y p(y)
\\     &amp; =  E[X] + E[Y] 
\end{align*}\]</span></p>
<p>Если <span class="math inline">\(X \perp Y\)</span> (это значит, что все события вида <span class="math inline">\(X \in A\)</span> независимы всем событиям вида <span class="math inline">\(Y \in B\)</span>), то еще и произведение можно снимать:</p>
<p><span class="math display">\[ E[X \cdot Y] = E[X] \cdot E[Y] \]</span></p>
<p>% Дисперсия суммы, произведения, домноженя на константу?</p>
<p>Найдите мат. ожидания и дисперсии следующих случайных величин:</p>
<ol type="a">
<li><p><span class="math inline">\(X = 1\)</span></p></li>
<li><p>$X =</p>
<span class="math display">\[\begin{cases} 
 0, &amp; p \\
 1, &amp; 1-p 
\end{cases}\]</span>
<p>$</p></li>
<li><p><span class="math inline">\(X \sim U[0, 1]\)</span> (<span class="math inline">\(X\)</span> равномерно распределена на отрезке <span class="math inline">\([0, 1]\)</span>)</p></li>
<li><p>Случайная величина из задачи 4.2 (геометрическое распределение) [10-11 классам надо честно найти, 8-9 классам можно погуглить чему равна сумма ряда, так как для его нахождения нужно уметь дифференцировать]</p></li>
</ol>
<p>Докажите, что <span class="math display">\[D[X] = E[X^2] - E[X]^2\]</span></p>
<p>Кинули кубик <span class="math inline">\(N\)</span> раз, найдите мат. ожидание числа выпавших шестерок.</p>
Подсказка: случайную величину <span class="math inline">\(X[\)</span>количество выпавших шестерок<span class="math inline">\(]\)</span> можно представить как <span class="math display">\[X = I_1 + ... + I_N\]</span> где $I_k =
<span class="math display">\[\begin{cases} 
    1, &amp; $на k-м кубике выпало $ 6 \\
    0, &amp; $иначе$ 
\end{cases}\]</span>
<p>$ Подсказка 2: используйте после этого линейность мат. ожидания</p>
<p>Взяли <span class="math inline">\(N\)</span> вершин и каждую пару разных вершин соединили с вероятностью <span class="math inline">\(0.5\)</span>. Найдите</p>
<ol type="a">
<li><p>мат. ожидание числа ребер</p></li>
<li><p>мат. ожидание числа треугольников</p></li>
</ol>
<p>Взяли <span class="math inline">\(N\)</span> вершин и каждую пару разных вершин соединили с вероятностью <span class="math inline">\(0.5\)</span>. Найдите</p>
<ol type="a">
<li><p>дисперсию числа ребер</p></li>
<li><p>дисперсию числа треугольников</p></li>
</ol>
<p>% ты знаешь доказательство «на пальцах»? % доказательство чего? ЦПТ % может не надо? может в следующий раз? % Ну, хоть как-то убедить в правильности % Там, про выборы и соцопросы рассказать % мне кажется, это можно упомянуть и не доказывать</p>
<p>% кстати, зацени http://sereja.me/a/pollard</p>
<p>%Метод Монте-карло и его относительная ошибка.</p>
<p>Есть замечательное нормальное распределение <span class="math inline">\(N(a, \sigma^2)\)</span> — это распределение со страшной плотностью <span class="math display">\[\rho(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-{\frac{(x-a)^2}{2\sigma^2}}}\]</span> Причем его мат. ожидание равно <span class="math inline">\(a\)</span>, а дисперсия равна <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Оно замечательно тем, что если взять много независимых случайных величин одного любого распределения <span class="math inline">\(X\)</span>, то распределение их среднего арифметического примерно равно <span class="math inline">\(N(E[X], \frac{D[X]}{N})\)</span>. Этот факт (более строгий, конечно) называется Центральной Предельной Теоремой.</p>
<p>Нарисуйте, как распределена (график плотности, или просто значения плотности) сумма значений на N игральных кубиках при</p>
<ol type="a">
<li><p>N = 1</p></li>
<li><p>N = 2</p></li>
<li><p>N = 3</p></li>
</ol>
<p>d*) N = 5</p>
<p>e*) N = 100</p>
<p>Подсказка: для 5 и 100 можно написать программу и нарисовать гистгорамму.</p>
Известно, что сумма двух нормально распределенных случайных величин — тоже нормально распределенная случайная величина. Пусть <span class="math inline">\(X \sim N(0, 1), Y \sim N(5, 4)\)</span>, найдите параметры (мат. ожидание и дисперсию) у случайной величины <span class="math inline">\(2X + Y\)</span>. 
<p>Про рандомизированные алгоритмы мы очень часто говорили фразу “в среднем работает за O(n)”. Это означает, что количество операций — это случайная величина, и её мат. ожидание — O(n).</p>
<p>Докажите, что Quicksort со случайным выбором опорного элемента на любой фиксированной перестановке работает в среднем за <span class="math inline">\(O(n\log{n})\)</span> сравнений.</p>
<p>Подсказка 1: разбейте количество сравнений на простые случайные величины и используйте линейность мат. ожидания.</p>
<p>Подсказка 2: докажите лемму — два числа <span class="math inline">\(x_i\)</span> и <span class="math inline">\(x_j\)</span> сравниваются только в том случае, если опорным элементом среди всех элементов между ними (в упорядоченном массиве) первым опорным элементом был выбран один из этих двух.</p>
<p>Найдите мат. ожидание глубины вершины случайного бинарного дерева (например, декартово дерево таким является), а именно такого дерева, у которого приоритеты вершин распределены одинаково, и по приоритетам дерево является кучей.</p>
<p>Подсказка: Докажите лемму — для любых <span class="math inline">\(i\neq k\)</span> , <span class="math inline">\(x_i\)</span> является предком <span class="math inline">\(x_k\)</span> тогда и только тогда, когда <span class="math inline">\(x_i\)</span> имеет наибольший приоритет среди <span class="math inline">\(x_{min(i, k)}\)</span>, $x_{min(i, k) + 1} … x_{max(i, k)} $.</p>
<p>Предполагая, что вероятность того, что число <span class="math inline">\(k\)</span> простое, равно <span class="math inline">\(\frac{1}{\ln k}\)</span>, покажите, что обычное решето Эратосфена работает за <span class="math inline">\(O(n \log \log n)\)</span>.</p>
%
<p>% Предполагая, что простые числа распределены как в прошлой задаче, «докажите» гипотезу Гольдбаха.</p>
<p>Дано корневое дерево. Каждую итерацию выбирается вершина (равновероятно из всех оставшихся), и удаляется всё поддерево, соответствующее этой вершине. Найти, сколько ходов в среднем будет продолжаться этот процесс, то есть матожидание номера итерации, на которой будет удалён корень дерева.</p>
<p>Пусть при мердже двух деревьев мы делаем подвешивание не за вершину с большим приоритетом, а следующим образом:</p>
<p>где <span class="math inline">\(L\)</span> и <span class="math inline">\(R\)</span> это размеры соответствующих деревьев.</p>
<p>Покажите, что каждая вершина всё так же равновероятно будет корнем дерева.</p>
%
%
<p>% Дан следующий код:</p>
<p>% % что-то не работает, как исправить? % \begin{lstlisting}[language=Python] % x = 0 % while x &lt; 1: % x += random() % \end{lstlisting}</p>
<p>% Требуется посчитать матожидание x.</p>
<p>% (random в питоне возвращает случайное действительное число от 0 до 1.)</p>
%
<p>% Пьяница стоит на краю обрыва (обрыв слева). С вероятностью <span class="math inline">\(p\)</span> он идёт на один шаг вправо, с вероятностью <span class="math inline">\(p-1\)</span> — на один шаг влево. Посчитайте вероятность того, что пьяница когда-либо упадёт.</p>
%
<p>% Вы стартуете с <span class="math inline">\(A\)</span> долларов и играете в азартную игру, постоянно делая ставку один доллар, которая дает вам два доллара с вероятностью <span class="math inline">\(p &lt; \frac{1}{2}\)</span>. Для оплаты смены в лагере Юнивёрсум вы хотите в какой-то момент получить <span class="math inline">\(B &gt; A\)</span> долларов, и как только это произойдет, вы играть прекратите. Найдите вероятность, что вы уйдете с деньгами. Вы либо уходите с <span class="math inline">\(B\)</span> долларами, либо разоряетесь. Возможно, вам придётся играть бесконечно.</p>
<h2 id="плотность-распределения">Плотность распределения</h2>
<p>Пусть есть некоторая случайная величина <span class="math inline">\(X\)</span>. <em>Плотностью распределения</em> <span class="math inline">\(X\)</span> называется функция <span class="math inline">\(p(x)\)</span>, отражающая относительную вероятность наступления события <span class="math inline">\(X=x\)</span> (то есть относительно всех остальных возможных событий). Например, для случайной величины с конечным множеством элементарных исходов <span class="math inline">\(\Omega\)</span> <span class="math inline">\(p(x)=P(X=x)\)</span> (где <span class="math inline">\(P(A)\)</span> - вероятность наступления события <span class="math inline">\(A\)</span>). Обычно функция плотности вероятности предполагается нормированной на единицу (то есть площадь под графиком равна 1). <em>Функцией распределения</em> случайной численной величины <span class="math inline">\(F(x)\)</span> называется вероятность попадания <span class="math inline">\(X\)</span> в луч <span class="math inline">\((-\infty;\,x]\)</span> (или <span class="math inline">\(\int_{-\infty}^{x}p(x)\,dx\)</span>).</p>
<h2 id="матожидание">Матожидание</h2>
<p>Предположим, у нас есть случайная численная величина <span class="math inline">\(X\)</span> с функцией плотности <span class="math inline">\(p(x)\)</span>. Тогда математическое ожидание случайной величины равно сумме <span class="math inline">\(P(A) \cdot A\)</span> для всех возможных исходов <span class="math inline">\(A\)</span>. TODO</p>
<h2 id="дисперсия">Дисперсия</h2>
<p>Какие два числа лучше всего описывают распределение?</p>
<h2 id="нормальное-распределение-1">Нормальное распределение</h2>
<p>Центральная предельная теорема названа так пафосно вполне обоснованно.</p>
<p>Она говорит, что нам достаточно про каждое слагаемое знать всего два числа — ожидание и дисперсию.</p>
<p><span class="math display">\[ f(x) = \frac{1}{2\sqrt{\pi}} e^{-\frac{(x-\mu)^2}{\sigma^2}} \]</span></p>
<p>Доказать это очень трудно. Даже со ссылками на мощные теоремы оно займёт не одну страницу. Обычно доказательство рассказывают в середине второго курса.</p>
<p>Трудно даже доказать, что это распределение, т. е. что.</p>
<h2 id="применения">Применения</h2>
<p>Пусть в некоторой стране есть два кандидата в президенты, назовём их Путин и Навальный.</p>
<p>Мы спросили у 1000 случайных избирателей бинарный вопрос, и 510 из них сказали, что будут голосовать за Путина. С какой вероятностью он победит? Теорема говорит, что число голосов, как</p>
<h2 id="линейные-рекурренты">Линейные рекурренты</h2>
<p>Чтобы решать следующие задачи, нам нужно будет использовать следующий факт:</p>
<p>…</p>
<p>Доказательство мы не приведем.</p>
<p>В частности, таким образом получается формула для чисел Фибоначчи.</p>
<p><span class="math display">\[ f_n = \ldots \]</span></p>
<p>Кто бы мог подумать, что все эти иррациональности и степени сократятся и вообще дадут целое число?..</p>
<h2 id="классика">Классика</h2>
<p>Парадокс дней рождения.</p>
<p>Это на самом деле очень часто используемый результат. Так можно считать вероятность коллизии хэшей, а также он используется во многих теоретико-числовых алгоритмах, в которых используется предположения (весьма справедливые) о распределении простых чисел.</p>
<p>Пьяница. Человек стоит на краю обрава и идёт в его сторону с вероятностью p. С какой вероятностью он когда-либо в него упадёт?</p>
<p>TODO: история про эстетическое удовольствие, азарт и смысл посещения казино. Казино. Мы приходим в казино с 1000$ и следующим образом проводим там время: ставим по 1$, пока не обанкротимся или не выиграем 1100$. Какая вероятность того, что мы уйдём с деньгами?</p>
<h2 id="парадокс-дней-рождений-1">Парадокс дней рождений</h2>
<blockquote>
<p>В группе, состоящей из 23 или более человек, вероятность совпадения дней рождения хотя бы у двух людей превышает 50%.</p>
</blockquote>
<p>Более общее утверждение: в мультимножество нужно добавить <span class="math inline">\(\Theta(\sqrt{n})\)</span> случайных чисел от 1 до n, чтобы какие-то два совпали.</p>
<p><strong>Первое доказательство</strong> (для любителей матана). Пусть <span class="math inline">\(f(n, d)\)</span> это вероятность того, что в группе из <span class="math inline">\(n\)</span> человек ни у кого не совпали дни рождения. Будем считать, что дни рождения распределены независимо и равномерно в промежутке от <span class="math inline">\(1\)</span> до <span class="math inline">\(d\)</span>.</p>
<p><span class="math display">\[
f(n, d) = (1-\frac{1}{d}) \times (1-\frac{2}{d}) \times ... \times (1-\frac{n-1}{d})
\]</span></p>
<p>Попытаемся оценить <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    e^x &amp; = 1 + x + \frac{x^2}{2!} + \ldots &amp; \text{(ряд Тейлора для экспоненты)} \\
    &amp; \simeq 1 + x &amp; \text{(аппроксимация для $|x| \ll 1$)} \\
    e^{-\frac{n}{d}} &amp; \simeq 1 - \frac{n}{d} &amp; \text{(подставим $\frac{n}{d} \ll 1$)} \\
    f(n, d) &amp; \simeq e^{-\frac{1}{d}} \times e^{-\frac{2}{d}} \times \ldots \times e^{-\frac{n-1}{d}} &amp; \\
    &amp; = e^{-\frac{n(n-1)}{2d}} &amp; \\
    &amp; \simeq e^{-\frac{n^2}{2d}} &amp; \\
\end{aligned}
\]</span></p>
<p>Из последнего выражения более-менее понятно, что вероятность <span class="math inline">\(\frac{1}{2}\)</span> достигается при <span class="math inline">\(n \approx \sqrt{d}\)</span> и в этой точке изменяется очень быстро.</p>
<p><strong>Второе доказательство</strong> (для любителей теорвера). Введем <span class="math inline">\(\frac{n(n-1)}{2}\)</span> индикаторов — по одному для каждой пары людей <span class="math inline">\((i, j)\)</span> — каждый будет равен единице, если дни рождения совпали. Ожидание и вероятность каждого индикатора равна <span class="math inline">\(\frac{1}{d}\)</span>.</p>
<p>Обозначим за <span class="math inline">\(X\)</span> число совпавших дней рождений. Его ожидание равно сумме ожиданий этих индикаторов, то есть <span class="math inline">\(\frac{n (n-1)}{2} \cdot \frac{1}{d}\)</span>.</p>
<p>Отсюда понятно, что если <span class="math inline">\(d = \Theta(n^2)\)</span>, то ожидание равно константе, а если <span class="math inline">\(d\)</span> асимптотически больше или меньше, то <span class="math inline">\(X\)</span> стремится нулю или бесконечности соответственно.</p>
<p><em>Примечание</em>: формально, из этого явно не следует, что вероятности тоже стремятся к 0 и 1.</p>
<h3 id="бонус-мета-задача">Бонус: «мета-задача»</h3>
<p>Дана произвольная строка, по которой известным только авторам задачи способом генерируется ответ yes/no. В задаче 100 тестов. У вас есть 20 попыток отослать решение. В качестве фидбэка вам доступны вердикты на каждом тесте. Вердиктов всего два: OK (ответ совпал) и WA. Попытки поделить на ноль, выделить терабайт памяти и подобное тоже считаются как WA.</p>
<p>«Решите» задачу.</p>
<h2 id="принцип-максимального-правдоподобия">Принцип максимального правдоподобия</h2>
<h2 id="энтропия">Энтропия</h2>
<p>Энтропией называется минимальное число бит, которым теоретически возможно сжать сообщение. Эта величина важна, потому что на практике если её можно посчитать, то сжатие с соответствующей кратностью реально достижимо.</p>
<p>Шумный канал.</p>
<p>Пусть у вас есть 1тб данных и два китайских терабайтника, на каждый из которых можно записать столько данных, но каждый бит имеет вероятность 10% записаться на противоположный. Требуется сохранить данные с первого раза без потерь. Совсем без потерь.</p>
<p>Причём это делается почти впритык.</p>
<h1 id="теорвер-для-алгоритмистов-1">Теорвер для алгоритмистов</h1>
<p>Вы, наконец, узнаете, почему ДД работает за логарифм, почему быстрая сортировка быстрая, какой модуль нужно выбирать для хэшей, сколько сэмплов нужно выбирать для монте-карло и т. д.</p>
<p>Следующие строчки позволят нам генерировать распределения и рисовать графики, не обращайте внимание.</p>
<h1 id="аксиоматика-1">Аксиоматика</h1>
<p>В вузах теорвер очень долго аксиоматизируют перед тем, как перейти к чему-то полезному. Сейчас поясним, зачем.</p>
<p>[Парадокс Бертрана](https://ru.wikipedia.org/wiki/%D0%9F%D0%B0%D1%80%D0%B0%D0%B4%D0%BE%D0%BA%D1%81_%D0%91%D0%B5%D1%80%D1%82%D1%80%D0%B0%D0%BD%D0%B0_(%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C%29):</p>
<blockquote>
<p>Рассмотрим равносторонний треугольник, вписанный в окружность. Наудачу выбирается хорда окружности. Какова вероятность того, что выбранная хорда длиннее стороны треугольника?</p>
</blockquote>
<p>Оказывается, можно придумать хотя бы три способа решать задачу, которые выглядят адекватно, но все дают разные ответы.</p>
<ol start="2" type="1">
<li><p>Наудачу выберем две точки на окружности и проведём через них хорду. Чтобы посчитать искомую вероятность, представим, что треугольник повёрнут так, что одна из его вершин совпадает с концом хорды. Заметим, что если другой конец хорды лежит на дуге между двумя другими вершинами треугольника, то длина хорды больше стороны треугольника. Длина рассмотренной дуги равна трети длины окружности, значит искомая вероятность равна <span class="math inline">\(\frac13\)</span>.</p></li>
<li><p>Зафиксируем радиус окружности, наудачу выберем точку на радиусе. Построим хорду, перпендикулярную зафиксированному радиусу, проходящую через выбранную точку. Для нахождения искомой вероятности, представим, что треугольник повёрнут так, что одна из его сторон перпендикулярна зафиксированному радиусу. Хорда длиннее стороны треугольника, если её центр ближе к центру, чем точка пересечения треугольника с зафиксированным радиусом. Сторона треугольника делит пополам радиус, следовательно вероятность выбрать хорду длиннее стороны треугольника <span class="math inline">\(\frac12\)</span>.</p></li>
<li><p>Выберем наудачу произвольную точку внутри круга и построим хорду с центром в выбранной точке. Хорда длиннее стороны равностороннего треугольника, если выбранная точка находится внутри круга, вписанного в треугольник. Площадь вписанного круга есть <span class="math inline">\(\frac14\)</span> от площади большего, значит исходная вероятность равна <span class="math inline">\(\frac14\)</span>.</p></li>
</ol>
<p>Как мы увидели, от формального определения «случайной хорды» непосредственно зависит ответ.</p>
<p>Каждый раз, когда в истории математики появляется подобный приводящий к протеворечиям парадокс, математики паникуют и начинают всё формализовывать и аксиоматизировать. Так появилась теория вероятностей (внимание: не «ти», а «тей»).</p>
<p>Перейдем к самим определениям:</p>
<p>Функцию <span class="math inline">\(X : \Omega \to R\)</span> будем называть случайной величиной. Она сопостав- ляет каждому элементарному исходу какое-то число.</p>
<h2 id="распределения">Распределения</h2>
<p><strong>Геометрическое распределение</strong>.</p>
<h2 id="парадокс-дней-рождений-2">Парадокс дней рождений</h2>
<p>Пусть <span class="math inline">\(f(n, d)\)</span> это вероятность того, что в группе из <span class="math inline">\(n\)</span> человек ни у кого не совпали дни рождения. Будем считать, что дни рождения распределены независимо и равномерно в промежутке от <span class="math inline">\(1\)</span> до <span class="math inline">\(d\)</span>.</p>
<p><span class="math display">\[f(n, d) = (1-\frac{1}{d}) \times (1-\frac{2}{d}) \times ... \times (1-\frac{n-1}{d})\]</span></p>
<p>Попытаемся оценить <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
\begin{align}
  \begin{aligned}
    e^x &amp; = 1 + x + \frac{x^2}{2!} + \ldots &amp; \text{(ряд Тейлора для экспоненты)} \\
    &amp; \simeq 1 + x &amp; \text{(аппроксимация для $|x| \ll 1$)} \\
    e^{-\frac{n}{d}} &amp; \simeq 1 - \frac{n}{d} &amp; \text{(подставим $\frac{n}{d} \ll 1$)} \\
    f(n, d) &amp; \simeq e^{-\frac{1}{d}} \times e^{-\frac{2}{d}} \times \ldots \times e^{-\frac{n-1}{d}} &amp; \\
    &amp; = e^{-\frac{n(n-1)}{2d}} &amp; \\
    &amp; \simeq e^{-\frac{n^2}{2d}} &amp; \\
  \end{aligned}
\end{align}
\]</span></p>
<h1 id="матожидание-1">Матожидание</h1>
<p>Математическим ожиданием случайной величины <span class="math inline">\(X\)</span>, которая принимает значения <span class="math inline">\(x_1, x_2, \ldots\)</span> называется</p>
<p><span class="math display">\[ E[X] = \sum_{x \in S} p_S(x) \cdot x \]</span></p>
<p>Самое главное для нас свойство — ожидание линейно:</p>
<p><span class="math display">\[
\begin{align*}
E[X+Y] &amp; = \sum_{x, y} (x+y) p(x, y)
\\     &amp; = \sum_{x, y} x p(x, y) + \sum_{x, y} y p(x, y)
\\     &amp; = \sum_x x p(x) \sum_y p(y) + \sum_y y p(y) \sum_x p(x)
\\     &amp; = \sum_x x p(x) + \sum_y y p(y)
\\     &amp; =  E[X] + E[Y] 
\end{align*}
\]</span></p>
<p>Акцентируем внимание: <span class="math inline">\(X\)</span> и <span class="math inline">\(Y\)</span> вообще не важно какие. Возможно, они связаны как-то очень сложно, но это не важно. Как мы потом, увидем, это свойство очень упрощает вычисление матожиданий каких-то очень сложных шняг.</p>
<p>Также, в частности его можно домножать на константу.</p>
<p>Теперь можно перейти к практическим задачам.</p>
<h2 id="геометрическое-распределение">Геометрическое распределение</h2>
<blockquote>
<p>Петя кидает монету, с вероятностью <span class="math inline">\(p\)</span> выпадает орел и он прекращает кидать ее, с вероятностью <span class="math inline">\(1-p\)</span> выпадает решка и он кидает ее еще раз. Пусть <span class="math inline">\(X\)</span> - это количество бросков. Найдите <span class="math inline">\(\rho_X(x)\)</span> и <span class="math inline">\(F_X(x)\)</span>.</p>
</blockquote>
<h2 id="неподвижные-точки-в-перестановке">Неподвижные точки в перестановке</h2>
<p>Когда автор пришел на первое занятие по английскому в МФТИ, препод устроила следующую игру на знакомство: разделила всех студентов на две команды. В каждой команде про своих членов загадываются факты: кто-то мечтал стать акробатом, кто-то смотрит анимэ и всё в таком духе. Команде соперников сообщались только эти факты, и им нужно было отгадать, кому какой принадлежит. Побеждает команда, которая отгадала больше фактов о соперниках. Нас было 11 — простое число, никак не делящееся на равные команды, и поэтому в одной команде было 5 человек, а в другой 6. Автору стало интересно: если отбросить все психологические аспекты и делать все рандомно, у какой команды вероятность победить выше?</p>
<p>Следует заметить, что все-таки ожидание правильно отгаданных вопросов не очень помогает выяснить, у какой команды есть преимущество и какое.</p>
<h2 id="высота-декартова-дерева">Высота декартова дерева</h2>
<p><strong>Высота дерева</strong>. Высота в среднем логарифмическая. Любознательные могут ознакомиться с типа доказательством, но это не обязательно.</p>
<p>Глубина вершины — это количество родителей (прим. К. О.). Введем <em>индикатор</em>. $E[h] =  n $.</p>
<h2 id="асимптотика-quicksort-а">Асимптотика quicksort-а</h2>
<p>Тут на самом деле будет примерно так же, как с ДД. Нас интересует суммарное число сравнений. Просуммируем для каждой пары элементов вероятность, что они будут сравнены.</p>
<p>Как для пары определить эту вероятность? Посмотрим на все элементы между ними, будь они в отсортированном массиве.</p>
<p>Внезапно возникает гармонический ряд, ну а дальше мы знаем.</p>
<h1 id="дисперсия-1">Дисперсия</h1>
<p>Дисперсия — это количественная метрика. Это не что-то абстрактное, как могли рассказать в школе на теорвере, а формально определенная величина, имеющая кучу всяких полезных свойств.</p>
<p>Дисперсия определяется как средний квадрат отклонения случайной величины от ее матожидания:</p>
<p><span class="math display">\[ D[X] = E[(X − E[X])^2] \]</span></p>
<p>Её проще считать по другой формуле, разложив квадрат внутри ожидания:</p>
<p><span class="math display">\[
\begin{align}
D[X] &amp;= E[(X − E[X])^2]
\\   &amp;= E[X^2 - 2 \cdot X \cdot E[X] + E[X]^2]
\\   &amp;= E[X^2] - E [\underbrace{2 \cdot E[X]}_{const} \cdot X ] + E[E[X]^2]
\\   &amp;= E[X^2] - 2 E[X]^2 + E[X]^2
\\   &amp;= E[X^2] - E[X]^2
\end{align}
\]</span></p>
<p>Эту формулу мы будем использовать для вывода разных свойств.</p>
<p>У дисперсии очень много крутых свойств.</p>
<p><span class="math display">\[ D[k X] = E[k^2 X^2] - E[k X]^2 = k^2 (E[X^2] - E[X]^2) = k^2 D[X] \]</span></p>
<p>Предполагаем, что <span class="math inline">\(X\)</span> и <span class="math inline">\(Y\)</span> независимы.</p>
<p><span class="math display">\[
\begin{align}
D[X + Y] &amp;= E[(X+Y)^2] - E[X+Y]^2
\\       &amp;= E[X^2 + X Y + Y^2] - (E[X] + E[Y])^2
\\       &amp;= E[X^2] + E[Y^2] - E[X]^2 - E[Y]^2
\\       &amp;= D[X] + D[Y]
\end{align}
\]</span></p>
<p>Важное отличие от свойств матожидания: дисперсию так просто можно считать только для независимых величин.</p>
<h2 id="закон-больших-чисел">Закон больших чисел</h2>
<p>Закон больших чисел — принцип, который описывает результат выполнения одного и того же эксперимента много раз. Согласно закону, среднее значение конечной выборки из фиксированного распределения близко к математическому ожиданию этого распределения.</p>
<p>У матожидания (константы) стандартное обозначение <span class="math inline">\(\mu\)</span> (мю), а у дисперсии <span class="math inline">\(\sigma\)</span> (сигма).</p>
<p><span class="math display">\[ M_n = \frac{X_1 + \ldots + X_n}{n} \]</span></p>
<p><span class="math display">\[ E[M_n] = \frac1n E[X_1 + \ldots + X_n] = \mu \]</span></p>
<p>Это немного очевидное равенство. Теперь нас интересует, насколько точно оно в реальности достигается:</p>
<p><span class="math display">\[ D[M_n] = \frac{1}{n^2} D[X_1 + \ldots + X_n] = \frac{\sigma}{n} \]</span></p>
<p>С одной стороны, оно домножается на <span class="math inline">\(\frac{1}[n^2}\)</span> из-за усреднения, с другой — на <span class="math inline">\(n\)</span> из-за суммирования.</p>
<h2 id="нормальное-распределение-2">Нормальное распределение</h2>
<p>Когда вы в каких-нибудь библиотеках для анализа данных просите показать какой-то summary, то чаще всего вы увидите два числа: матожидание и дисперсию.</p>
<p>Центральная предельная теорема названа так пафосно вполне обоснованно.</p>
<p>Она говорит, что нам достаточно про каждое слагаемое знать всего два числа — ожидание и дисперсию.</p>
<p><span class="math display">\[ f(x) = \frac{1}{2\sqrt{\pi}} e^{-\frac{(x-\mu)^2}{\sigma^2}} \]</span></p>
<p>Доказать это очень трудно. Даже со ссылками на мощные теоремы оно займёт не одну страницу. Обычно доказательство рассказывают в середине второго курса.</p>
<p>Трудно даже доказать, что это распределение, т. е. что <span class="math inline">\(\int_{-\inf}^\inf f(x) dx = 1\)</span>.</p>
<h2 id="метод-монте-карло">Метод Монте-Карло</h2>
<p>Алгоритмы вида «давайте посчитаем значения в разных случайных точках и усредним» называются методами монте-карло.</p>
<p>Пусть у нас есть единичный квадрат и несколько сотен кружков. Нам нужно посчитать долю области квадрата, которая не покрывается ни одним из кругов, с точностью до 1%.</p>
<p>Можно просто делать так: тыкать в 10000 случайных точек и проверять для каждой, является ли она «хорошей», а затем вывести <span class="math inline">\(\frac{\text{хорошие}}{\text{хорошие} + \text{плохие}}\)</span> в качестве ответа.</p>
<p>Сдать можно сюда.</p>
<h2 id="хэш-таблицы">Хэш-таблицы</h2>
<p>Вообще, хэш — это такая функция, которая сопоставляет элементу какой-то другой элемент. С точки зрения криптоанализа, она должна быть трудно обратимой, а с точки зрения алгоритмиста — генерирующей равномерные игреки.</p>
<p>Можно делать так если мы планируем хранить <span class="math inline">\(n\)</span> элементов, то нужно завести <span class="math inline">\(\Theta(n)\)</span> ячеек, каждая из которых будет на самом деле односвязным списокм.</p>
<p>Это будет работать, потому что из-за рандома в среднем в каждой ячейке будет по одному элементу. Но константа у такого решения большая.</p>
<p>Стандартная хэш-таблица из STL по непонятным автору причинам работает очень медленно. Во многих задачах она является самой нагруженной структурой. Её можно написать в 3-5 раз быстрее.</p>
</body>
</html>
